Continious action space with traditional TD3 algorithm 
Actions in range [-1,..., 1]
    -1 equals sell all
    1 equals buy all 
    0 means hold 

Discovered huge bug that made it so the incorrect state was fed into the network.
All previous experiments are invalid.

Code:

model.py
import copy
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import utility.utils as utils


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Implementation of Twin Delayed Deep Deterministic Policy Gradients (TD3)
# Paper: https://arxiv.org/abs/1802.09477
# Original Implementation found on https://github.com/sfujim/TD3/blob/master/TD3.py
class Encoder(nn.Module):
    def __init__(self, inchannel, state_length, immediate_state_dim, hidden_size, outchannel, activation=nn.ReLU):
        super(Encoder, self).__init__()
        self.layers = nn.Sequential(
            nn.Conv1d(inchannel, hidden_size, kernel_size=3, padding=1),
            nn.BatchNorm1d(hidden_size),
            activation(),
            nn.Conv1d(hidden_size, outchannel, kernel_size=3, padding=1),
            nn.BatchNorm1d(outchannel),
        )
        self.relu = activation()
        if inchannel == outchannel:
            self.shortcut = nn.Identity()
        else:
            self.shortcut = nn.Conv1d(inchannel, outchannel, kernel_size=1)
        self.output = nn.Linear(outchannel * state_length, outchannel)
    
    def forward(self, X, X_immediate):
        out =  self.layers(X)
        shortcut = self.shortcut(X)
        out = self.relu(out + shortcut)
        shape = out.shape
        out = out.reshape((shape[0], shape[1] * shape[2]))
        out = self.output(out)   
        return out

class Actor(nn.Module):
    def __init__(self, ind_state_dim, ind_state_length, imm_state_dim, action_dim, max_action):
        super(Actor, self).__init__()
        self.conv = Encoder(ind_state_dim, ind_state_length, imm_state_dim, 64, 64)
        self.l1 = nn.Linear(64 , 64)
        self.l2 = nn.Linear(64, 64)
        self.l3 = nn.Linear(64, action_dim)

        self.max_action = max_action


    def forward(self, ind_state, imm_state):
        ind_state = self.conv(ind_state, imm_state)
        a = F.relu(self.l1(ind_state))
        a = F.relu(self.l2(a))
        a = self.max_action * torch.tanh(self.l3(a))
        return a


class Critic(nn.Module):
    def __init__(self, indicator_state_dim, state_length, immediate_state_dim, action_dim):
        super(Critic, self).__init__()

        # Q1 architecture
        self.conv = Encoder(indicator_state_dim, state_length, immediate_state_dim, 64, 64)
        self.l1 = nn.Linear(64 + action_dim, 64)
        self.l2 = nn.Linear(64, 64)
        self.l3 = nn.Linear(64, 1)

        # Q2 architecture
        self.conv2 = Encoder(indicator_state_dim, state_length, immediate_state_dim, 64, 64)
        self.l4 = nn.Linear(64 + action_dim, 64)
        self.l5 = nn.Linear(64, 64)
        self.l6 = nn.Linear(64, 1)

    def forward(self, indicator_state_dim, immediate_state_dim, action):
        sa1 = self.conv(indicator_state_dim, immediate_state_dim)
        sa1 = torch.cat([sa1, action], 1)
        q1 = F.relu(self.l1(sa1))
        q1 = F.relu(self.l2(q1))
        q1 = self.l3(q1)

        sa2 = self.conv2(indicator_state_dim, immediate_state_dim)
        sa2 = torch.cat([sa2, action], 1)
        q2 = F.relu(self.l4(sa2))
        q2 = F.relu(self.l5(q2))
        q2 = self.l6(q2)
        return q1, q2

    def Q1(self, indicator_state_dim, immediate_state_dim, action):
        sa = self.conv(indicator_state_dim, immediate_state_dim)
        sa = torch.cat([sa, action], 1)
        q1 = F.relu(self.l1(sa))
        q1 = F.relu(self.l2(q1))
        q1 = self.l3(q1)
        return q1


class TD3(object):
    def __init__(
        self,
        state_dim,
        action_dim,
        max_action,
        discount=0.99,
        tau=0.005,
        policy_noise=0.2,
        noise_clip=0.5,
        policy_freq=2,
        lr=3e-4
    ):
        indicator_state_dim, immediate_state_dim = state_dim
        self.actor = Actor(indicator_state_dim[0], indicator_state_dim[1], immediate_state_dim[0], action_dim, max_action).to(device)
        self.actor_target = copy.deepcopy(self.actor)
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr)

        self.critic = Critic(indicator_state_dim[0], indicator_state_dim[1], immediate_state_dim[0], action_dim).to(device)
        self.critic_target = copy.deepcopy(self.critic)
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=lr)

        self.max_action = max_action
        self.discount = discount
        self.tau = tau
        self.policy_noise = policy_noise
        self.noise_clip = noise_clip
        self.policy_freq = policy_freq

        self.total_it = 0

    def select_action(self, state_tup):
        ind_state, imm_state = state_tup
        if (len(ind_state.shape) == 2):
            ind_state = torch.FloatTensor([ind_state]).to(device)
            imm_state = torch.FloatTensor([imm_state]).to(device)
        else:
            ind_state = torch.FloatTensor(ind_state).to(device)
            imm_state = torch.FloatTensor(imm_state).to(device)
        action = self.actor(ind_state, imm_state).cpu().data.numpy()
        return action

    def train(self, replay_buffer, batch_size=100):
        self.total_it += 1

        # Sample replay buffer
        ind_state, imm_state, action, next_ind_state, next_imm_state, reward, not_done = replay_buffer.sample(batch_size)

        with torch.no_grad():
            # Select action according to policy 
            noise = (torch.randn_like(action) * self.policy_noise).clamp(
                -self.noise_clip, self.noise_clip
            )

            next_action = (self.actor_target(next_ind_state, next_imm_state) + noise).clamp(
                -self.max_action, self.max_action
            )

            # Compute the target Q value
            target_Q1, target_Q2 = self.critic_target(next_ind_state, next_imm_state, next_action)
            target_Q = torch.min(target_Q1, target_Q2)
            target_Q = reward + not_done * self.discount * target_Q

        # Get current Q estimates
        current_Q1, current_Q2 = self.critic(next_ind_state, next_imm_state, action)

        # Compute critic loss
        critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(
            current_Q2, target_Q
        )

        # Optimize the critic
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()

        # Delayed policy updates
        if self.total_it % self.policy_freq == 0:

            # Compute actor losse
            actor_loss = -self.critic.Q1(next_ind_state, next_imm_state, 
                                self.actor(next_ind_state, next_imm_state)).mean()

            # Optimize the actor
            self.actor_optimizer.zero_grad()
            actor_loss.backward()
            self.actor_optimizer.step()

            # Update the frozen target models
            for param, target_param in zip(
                self.critic.parameters(), self.critic_target.parameters()
            ):
                target_param.data.copy_(
                    self.tau * param.data + (1 - self.tau) * target_param.data
                )

            for param, target_param in zip(
                self.actor.parameters(), self.actor_target.parameters()
            ):
                target_param.data.copy_(
                    self.tau * param.data + (1 - self.tau) * target_param.data
                )

    def save(self, filename):
        torch.save(self.critic.state_dict(), filename + "_critic")
        torch.save(self.critic_optimizer.state_dict(), filename + "_critic_optimizer")

        torch.save(self.actor.state_dict(), filename + "_actor")
        torch.save(self.actor_optimizer.state_dict(), filename + "_actor_optimizer")

    def load(self, filename):
        self.critic.load_state_dict(torch.load(filename + "_critic"))
        self.critic_optimizer.load_state_dict(
            torch.load(filename + "_critic_optimizer")
        )
        self.critic_target = copy.deepcopy(self.critic)

        self.actor.load_state_dict(torch.load(filename + "_actor"))
        self.actor_optimizer.load_state_dict(torch.load(filename + "_actor_optimizer"))
        self.actor_target = copy.deepcopy(self.actor)


class ReplayBuffer(object):
    def __init__(self, state_dim, action_dim, max_size=int(1e6)):
        indicator_state, immediate_state = state_dim
        self.max_size = max_size
        self.ptr = 0
        self.size = 0
        if type(indicator_state) == int:
            full_indicator_state = [max_size] + [indicator_state]
        else:
            full_indicator_state = [max_size] + [s for s in indicator_state]
        self.action = np.zeros((max_size, action_dim))
        self.indicator_state = np.zeros(full_indicator_state)
        self.immediate_state = np.zeros((max_size, immediate_state[0]))
        self.next_indicator_state = np.zeros(full_indicator_state)
        self.next_immediate_state = np.zeros((max_size, immediate_state[0]))
        self.reward = np.zeros((max_size, 1))
        self.not_done = np.zeros((max_size, 1))
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


    def add(self, 
            state, 
            action, 
            next_state,
            reward, 
            done):
        indicator_state, immediate_state = state 
        next_indicator_state, next_immediate_state = next_state 

        self.immediate_state[self.ptr] = immediate_state
        self.indicator_state[self.ptr] = indicator_state
        self.action[self.ptr] = action
        self.next_immediate_state[self.ptr] = next_immediate_state
        self.next_indicator_state[self.ptr] = next_indicator_state
        self.reward[self.ptr] = reward
        self.not_done[self.ptr] = 1. - done
        self.ptr = (self.ptr + 1) % self.max_size
        self.size = min(self.size + 1, self.max_size)
    
    def sample(self, batch_size):
        ind = np.random.randint(0, self.size, size=batch_size)

        return (
            torch.FloatTensor(self.indicator_state[ind]).to(self.device),
            torch.FloatTensor(self.immediate_state[ind]).to(self.device),
            torch.FloatTensor(self.action[ind]).to(self.device),
            torch.FloatTensor(self.next_indicator_state[ind]).to(self.device),
            torch.FloatTensor(self.next_immediate_state[ind]).to(self.device),
            torch.FloatTensor(self.reward[ind]).to(self.device),
            torch.FloatTensor(self.not_done[ind]).to(self.device)
        )

state.py
import pandas as pd
import numpy as np
import datetime


class State(object):
    """
    Represents the internal state of an environment
    """
    def __init__(self, stock_names, starting_money, starting_shares, current_date, current_time, days_in_state=100):
        """
        Initializes the State of the environment

        Parameter stock_name: the name of the stocks for the state.
        Precondition: stock_names must be an array of stocks or ETFs

        Parameter starting_money: the initial amount of buying power for the state
        Precondition: starting_money must be an array of buying power or ETFs

        Parameter starting_shares: the initial amount of shares for the state
        Precondition: starting_shares must be an array of stocks or ETFs

        Parameter current_date: the current date of the state.
        Precondition: current_date must be a string in this format: YYYY-DD-MM

        Parameter current_time: the current time of the state.
        Precondition: current_time must be a string in this format: HH:MM
        """
        self.dataframes = dict()
        self.stock_names = stock_names
        self.number_of_stocks = len(stock_names)
        self.days_in_state = days_in_state
        
        if type(stock_names) == str:
            stock_names = [stock_names]
        for stock_name in stock_names:
            filename = f"data/price_data/{stock_name}.csv"
            try:
                self.dataframes[stock_name] = pd.read_csv(filename, index_col="Date")
            except:
                raise AssertionError(stock_name + " is not a stock or ETF.")
        self.essential_state = np.concatenate([
            starting_money, starting_shares, self.get_stock_prices(current_date, current_time)
        ])
        self.past_state = PastState(len(self.essential_state), days_in_state)
        # self.past_state.add(self.essential_state)
        self.get_indicators()
        self.indicator_state = self.get_indicator_state(current_date, current_time)
        state1, state2 = self.get_state()
        self.shape = (state1.shape, state2.shape)
      
    
    def get_indicator_state(self, current_date, current_time):
        """
        Returns: The past 'days' of the indicator state
        """
        date_arr = [int(x) for x in current_date.split('-')]
        date_obj = datetime.date(date_arr[0], date_arr[1], date_arr[2]) - datetime.timedelta(self.days_in_state)
        past_date = str(date_obj)
        result = []
        for stock in self.stock_names:
            data = self.dataframes[stock].copy().loc[past_date: current_date]

            if current_time == 'Open':
                # We do not know the High, Low, Close, or indicators of the current date at open 
                # We must zero them out so the agent is not looking at the future
                open_price = data.loc[current_date]['Open']
                data.loc[current_date] = 0
                data.loc[current_date]['Open'] = open_price
            # print("data", data)
            data_as_numpy = data.to_numpy()        
            result.append(data_as_numpy)

        return np.array(result)

    def get_stock_prices(self, current_date, current_time):
        """
        Gets the current stock price at this epoch
        """
        result = []
        for stock in self.stock_names:
            price = self.dataframes[stock].loc[current_date][current_time]
            result.append(price)
        return np.array(result)
    
    def get_new_holdings(self, action, stock_prices):
        """
        Returns: the new holdings after performing action in the current state
        """
        old_holdings = self.essential_state[1 : 1 + self.number_of_stocks]
        current_cash = self.essential_state[0]
        new_holdings = []
        invalid_action = False
        for a, holding, price in zip(action, old_holdings, stock_prices):
            if a > 0:
                cash = a * current_cash / len(old_holdings)
                shares = cash / price
                total_price = shares * price
                current_cash -= total_price 
                new_holdings.append(holding + shares)
                
            else:
                shares = abs(a) * holding 
                total_price = shares * price
                current_cash += total_price 
                new_holdings.append(holding - shares)
        return np.array(new_holdings), current_cash, invalid_action
    
    def get_holdings(self):
        """
        Returns: the current holdings
        """
        return self.essential_state[1:1+self.number_of_stocks]
    
    def calculate_portfolio_value(self):
        """
        Returns: the current portfolio value
        """
        return self.essential_state[0] + np.sum(
            self.essential_state[1 : 1 + self.number_of_stocks]
            * self.essential_state[1 + self.number_of_stocks : 1 + 2 * self.number_of_stocks]
        )
    
    def advance_state(self, remaining_money, holdings, current_date, current_time):
        """
        Advances the state to the next state

        Parameter remaining_money (int): The buing power in the new state
        Parameter holdings (int[]): The holdings of each stock in the state
        Parameter current_date (string): The date of the new state
        Parameter current_time (string): The time of the new state
        """
        # if current_time == 'Close':
        #     self.past_state.add(self.essential_state)
        stock_prices = self.get_stock_prices(current_date, current_time)
        self.essential_state = np.concatenate([
            np.array([remaining_money]), holdings, stock_prices
        ])  
        self.indicator_state = self.get_indicator_state(current_date, current_time)


    
    def get_indicators(self):
        """
        Adds indicators to the dataframe
        """
        for stock in self.stock_names:
            # get MACD
            df = self.dataframes[stock]
            exp1 = df.ewm(span=12, adjust=False).mean()
            exp2 = df.ewm(span=26, adjust=False).mean()
            macd = exp1 - exp2
            df['macd'] = macd['Close']

            # get moving averages
            df["seven_day_mean_moving_average"] = df.rolling(window=7).mean()['Close']
            df["thirty_day_mean_moving_average"] = df.rolling(window=30).mean()['Close']
            df["ninety_day_mean_moving_average"] = df.rolling(window=90).mean()['Close']
            df["two_hundred_day_mean_moving_average"] = df.rolling(window=200).mean()['Close']

            df["seven_day_std_moving_average"] = df.rolling(window=7).std()['Close']
            df["thirty_day_std_moving_average"] = df.rolling(window=30).std()['Close']
            df["ninety_day_std_moving_average"] = df.rolling(window=90).std()['Close']
            df["two_hundred_day_std_moving_average"] = df.rolling(window=200).std()['Close']

            # get bollander bands
            df["upper_bolliander_band"] = df.rolling(window=20).mean()['Close'] + 2 * df.rolling(window=20).std()['Close']
            df["lower_bolliander_band"] = df.rolling(window=20).mean()['Close'] - 2 * df.rolling(window=20).std()['Close']
            
            # get rsi
            diff = df['Close'].diff(1).dropna()       
            up_chg = 0 * diff
            down_chg = 0 * diff
            
            up_chg[diff > 0] = diff[ diff>0 ]            
            down_chg[diff < 0] = diff[ diff < 0 ]

            up_chg_avg   = up_chg.ewm(com=13 , min_periods=14).mean()
            down_chg_avg = down_chg.ewm(com=13 , min_periods=14).mean()
            
            rs = abs(up_chg_avg/down_chg_avg)
            rsi = 100 - 100/(1+rs)
            df['rsi'] = rsi
            self.dataframes[stock] = self.dataframes[stock].dropna()
        

    
    def reset(self, starting_money, starting_shares, current_date, current_time):
        """
        Resets the state with the new parameters
        """
        self.essential_state = np.concatenate([
            starting_money, starting_shares, self.get_stock_prices(current_date, current_time)
        ])
        # self.past_state.reset()
        # self.past_state.add(self.essential_state)
    
    def to_numpy(self):
        """
        Returns the numpy array representing the state object

        Alias for self.get_state()
        """
        return self.get_state()

    def get_state(self):
        """
        Returns: the internal array representing the state
        """
        num_stocks, length, num_indicators = self.indicator_state.shape
        reshaped_indicator_state = self.indicator_state.reshape((length, num_stocks * num_indicators))
        length = len(reshaped_indicator_state)
        reshaped_indicator_state = reshaped_indicator_state[length - int(0.6 * self.days_in_state):length]
        return reshaped_indicator_state, self.essential_state



class PastState(object):
    """
    Represents the past state of State
    """
    def __init__(self, days_in_state, max_size):
        """
        Initializes the past state
        """
        self.max_size = max_size
        self.days_in_state = days_in_state
        self.reset()
    
    def __len__(self):
        """
        Returns: The length of the state
        """
        return len(self.data)
    
    def __getitem__(self, *args):
        """
        Returns: get item of the past state
        """
        return self.data.__getitem__(args)

    def reset(self):
        """
        Resets the state to the initial state
        """
        self.data = np.zeros((self.max_size, self.days_in_state))
        self.current_size = 0
        self.shape = self.data.shape
        
    
    def add(self, essential_state):
        """
        Adds the state to the past state queue
        """
        if self.current_size < self.max_size:
            self.data[self.max_size - self.current_size - 1] = essential_state
            self.current_size += 1
        else:
            self.data = np.vstack((essential_state, self.data[:-1]))
    
    def copy(self):
        """
        Returns a copy of the internal state
        """
        return self.data.copy()
        

environment.py

import gym
from gym import spaces
import pandas as pd
import numpy as np
from tqdm import tqdm
from models.alternative.model import TD3, ReplayBuffer
from models.alternative.state import State
import random
import re
import datetime


class StockEnv(gym.Env):
    """
    The current environment of the agent.

    The environment keeps track of where the agent is after taking action a in 
    state s.
    """
    def __init__(
        self,
        stock_names,
        start_date,
        end_date,
        max_limit,
        starting_amount_lower=20000,
        starting_amount_upper=50000,
        random_start=False,
        invalid_action_penalty=5
    ):
        """
        Initializes the environment.
        
        Parameter stock_names: the name of the stocks for this environment.
        Precondition: stock_names must be an array of stocks or ETFs

        Parameter start_date: the starting date of this environment.
        Precondition: start_date must be a string in this format: YYYY-DD-MM

        Parameter end_date: the ending date of this environment.
        Precondition: end_date must be a string in this format: YYYY-DD-MM and 
                    end_date must be after start_date
        """
        super(StockEnv, self).__init__()
        self.random_start = random_start
        self.valid_dates = pd.read_csv("data/price_data/SPY.csv", index_col="Date").index
        
        self.number_of_stocks = len(stock_names)
        self.stock_names = stock_names
        self.initialize_date(start_date, end_date), "Date preconditions failed"
        self.starting_amount_lower = starting_amount_lower
        self.starting_amount_upper = starting_amount_upper
        self.starting_amount = self.starting_amount_upper
        self.reset(init=True)

        self.action_space = spaces.Box(
            low=-max_limit, high=max_limit, shape=(self.number_of_stocks,), dtype=np.float32
        )
        self.invalid_action_penalty = invalid_action_penalty

    def calculate_reward(self, holdings, remaining_money, stock_prices_new, action_is_invalid=False):
        value_last = self.value_at_last_timestep
        r = (
            remaining_money
            + np.sum(holdings * (stock_prices_new))
        )
        self.value_at_last_timestep = r
        if action_is_invalid:
            r = r - self.invalid_action_penalty # can penalize invalid actions
        return r - value_last

    def step(self, action):
        """
        Takes action in the current state to get to the next state

        Returns an array [new_state, reward, done] where:
            - new_state (State object): state after taking action in the current state
            - reward (float): reward for taking action in the current state 
            - done (boolean): whether or not the run is done 
        """

        current_date, current_time = self.get_date_and_time()
        stock_prices_old = self.state.get_stock_prices(current_date, current_time)
        # perform action: if buying, add positions. if selling, subtract positions.
        # change buying power
        holdings, remaining_money, action_is_invalid = self.state.get_new_holdings(action, stock_prices_old)
        self.increment_date()
        new_date, new_time = self.get_date_and_time()
        stock_prices_new = self.state.get_stock_prices(new_date, new_time)
        self.state.advance_state(remaining_money, holdings, new_date, new_time)
        reward = self.calculate_reward(holdings, remaining_money, stock_prices_new, action_is_invalid)
        return self.state, reward, self.is_done()
        

    def increment_date(self):
        """
        Increments the date by one epoch
        """
        incr = 1
        start_arr = list(map(lambda x: int(x), re.split(r"[\-]", self.start_date)))
        date_obj = datetime.date(start_arr[2], start_arr[0], start_arr[1])
        s = self.stock_names[0]
        adjusted_date = str(date_obj + datetime.timedelta((self.epochs + incr) // 2))
        while not (
            adjusted_date
            in self.valid_dates
        ):
            incr += 1
            adjusted_date = str(date_obj + datetime.timedelta((self.epochs + incr) // 2))
            if incr >= 20:
                raise Exception(f"{adjusted_date} is out of range")
        self.epochs += incr

    def is_done(self):
        """
        Returns: True if the episode is done. False otherwise
        """
        return self.epochs >= self.max_epochs 

    def reset(self, init=False):
        """
        Resets the environment to a random date in the first 33% of the range 
        with a random amount of positions and random amount of buying power
        """
        if self.random_start:
            starting_money = [random.randint(
                self.starting_amount_lower, self.starting_amount_upper
            )]
            starting_shares = [
                random.randint(0, 10) for _ in range(self.number_of_stocks)
            ]
        else:
            starting_money = [self.starting_amount_upper]
            starting_shares = [0 for _ in range(self.number_of_stocks)]
        starting_money = np.array(starting_money)
        starting_shares = np.array(starting_shares)
        self.value_at_last_timestep = 0
        self.initialize_starting_epoch(self.start_date, self.end_date)
        
        current_date, current_time = self.get_date_and_time()
        if init:
            self.state = State(self.stock_names, starting_money, starting_shares, current_date, current_time)
        else:
            self.state.reset(starting_money, starting_shares, current_date, current_time)
        self.starting_amount = self.state.calculate_portfolio_value()
        return self.state

    
    def get_date_and_time(self):
        """
        Gets current date and time
        """
        time = "Open" if self.epochs % 2 == 0 else "Close"
        start_arr = list(map(lambda x: int(x), re.split(r"[\-]", self.start_date)))
        date_obj = datetime.date(
            start_arr[2], start_arr[0], start_arr[1]
        ) + datetime.timedelta(self.epochs // 2)
        return str(date_obj), time

    def calculate_portfolio_value(self):
        """
        Calculates the current portfolio value
        """
        return self.state.calculate_portfolio_value()
    
    def get_holdings(self):
        """
        Returns: the current holdings
        """
        return self.state.get_holdings()

    def initialize_date(self, start_date, end_date):
        """
        Returns: True if start_date and end_date are in the right format.
                False otherwise
        """
        start_arr = re.split(r"[\-]", start_date)
        end_arr = re.split(r"[\-]", end_date)
        date_is_valid = True
        for x, y in zip(start_arr, end_arr):
            date_is_valid = x.isdigit() and y.isdigit() and date_is_valid
            if date_is_valid:
                date_is_valid = date_is_valid and int(x) > 0 and int(y) > 0
            else:
                return date_is_valid
        date1 = [int(x) for x in re.split(r"[\-]", start_date)]
        date2 = [int(x) for x in re.split(r"[\-]", end_date)]
        date1_obj = datetime.date(date1[2], date1[0], date1[1])
        date2_obj = datetime.date(date2[2], date2[0], date2[1])
        epochs = (date2_obj - date1_obj).days
        if not (date_is_valid and epochs >= 0):
            raise ValueError("Date is not valid")
        self.max_epochs = epochs * 2
        self.start_date = start_date
        self.end_date = end_date

    def initialize_starting_epoch(self, start_date, end_date):
        """
        Gets the starting epoch of a cycle
        """
        if self.random_start:
            date1 = [int(x) for x in re.split(r"[\-]", start_date)]
            date2 = [int(x) for x in re.split(r"[\-]", end_date)]
            date1_obj = datetime.date(date1[2], date1[0], date1[1])
            date2_obj = datetime.date(date2[2], date2[0], date2[1])
            self.epochs = random.randint(-1, int((date2_obj - date1_obj).days * 0.2))
        else:
            self.epochs = -1
        self.increment_date()  # needed to be sure we're not on a weekend/holiday
